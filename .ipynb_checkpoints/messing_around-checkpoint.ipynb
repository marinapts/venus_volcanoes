{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "from src.pyvov import ChipsIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets():\n",
    "    \"\"\"\n",
    "    This takes ALL of the available datapoints and makes train/val/test splits.\n",
    "    Needs to be improved upon by looking at specific experiments e.g HOM36 etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = DataLoader()\n",
    "    X_train = data.get_training_set()\n",
    "    X_train = np.asarray(X_train)\n",
    "    \n",
    "    X_val = data.get_validation_set()\n",
    "    X_val = np.asarray(X_val)\n",
    "    \n",
    "    X_test = data.get_testing_set()\n",
    "    X_test = np.asarray(X_test)\n",
    "    \n",
    "    labels = data.get_labels()\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    new_labels = np.zeros_like(labels)\n",
    "    for idx, item in enumerate(labels):\n",
    "        if item > 0:\n",
    "            new_labels[idx] += 1\n",
    "\n",
    "    \n",
    "    n = new_labels.shape[0]\n",
    "    y_val = new_labels[0:int(0.1*n)]\n",
    "    y_test = new_labels[int(0.1*n):int(0.2*n)]\n",
    "    y_train = new_labels[int(0.2*n):]\n",
    "    \n",
    "    print('X_train shape:', X_train.shape, '   y_train shape:', y_train.shape)\n",
    "    print('X_val shape:', X_val.shape, '   y_val shape:', y_val.shape)\n",
    "    print('X_test shape:', X_test.shape, '   y_test shape:', y_test.shape)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "            \n",
    "def principal_components(X, y, n):\n",
    "    \n",
    "    positives = X[np.where(y>0)]\n",
    "    \n",
    "    pca = PCA(n_components=n)\n",
    "    pcs = pca.fit(positives)\n",
    "    \n",
    "    return pca\n",
    "\n",
    "def normalize(X):\n",
    "    X = X.astype(float)\n",
    "    new_X = np.zeros_like(X)\n",
    "    \n",
    "    for idx in range(X.shape[0]):\n",
    "        new_X[idx] = X[idx] - np.mean(X[idx])\n",
    "        new_X[idx] = new_X[idx]/255.0\n",
    "        \n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gaussian_clf():\n",
    "    def __init__(self, threshold=0.5, normalize=False):\n",
    "        self.threshold=threshold\n",
    "        self.normalize=normalize\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \n",
    "        if self.normalize:\n",
    "            X_train = normalize(X_train)\n",
    "        \n",
    "        self.pca = principal_components(X_train, y_train, 6)\n",
    "        X_train = self.pca.transform(X_train)\n",
    "        \n",
    "        positives = X_train[np.where(y_train>0)]\n",
    "        negatives = X_train[np.where(y_train==0)]\n",
    "        assert positives.shape[0]+negatives.shape[0]==X_train.shape[0]\n",
    "        \n",
    "        self.mean_0 = negatives.mean(axis=0)\n",
    "        self.cov_0 = np.cov(negatives.T)\n",
    "        self.mean_1 = positives.mean(axis=0)\n",
    "        self.cov_1 = np.cov(positives.T)\n",
    "        \n",
    "        self.prior = positives.shape[0] / (positives.shape[0] + negatives.shape[0])\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        self.score_0 = multivariate_normal.pdf(X, mean=self.mean_0, cov=self.cov_0)\n",
    "        self.score_1 = multivariate_normal.pdf(X, mean=self.mean_1, cov=self.cov_1)\n",
    "        posterior = self.score_1 * self.prior / (self.score_1 * self.prior + self.score_0 * (1-self.prior))\n",
    "        \n",
    "        return posterior\n",
    "        \n",
    "    def evaluate(self, X, y):\n",
    "        if self.normalize:\n",
    "            X = normalize(X)\n",
    "        \n",
    "        X = self.pca.transform(X)\n",
    "        true_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "        \n",
    "        for idx in range(X.shape[0]):\n",
    "            if self.predict_prob(X[idx]) > self.threshold:\n",
    "                if y[idx]==1:\n",
    "                    true_positives += 1\n",
    "                else:\n",
    "                    false_positives += 1\n",
    "            \n",
    "            else:\n",
    "                if y[idx]==1:\n",
    "                    false_negatives += 1\n",
    "                else:\n",
    "                    true_negatives += 1\n",
    "        \n",
    "        print('true_positives:', true_positives)\n",
    "        print('true_negatives:', true_negatives)\n",
    "        print(\"false_positives:\", false_positives)\n",
    "        print(\"false_negatives:\", false_negatives)\n",
    "        \n",
    "        print(\"detected volcanoes:\", true_positives/(true_positives+false_negatives))\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (29824, 225)    y_train shape: (29824,)\n",
      "X_val shape: (3728, 225)    y_val shape: (3728,)\n",
      "X_test shape: (3728, 225)    y_test shape: (3728,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = get_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives: 17\n",
      "true_negatives: 3552\n",
      "false_positives: 39\n",
      "false_negatives: 120\n"
     ]
    }
   ],
   "source": [
    "baseline = gaussian_clf(threshold=0.35, normalize = True)\n",
    "baseline.train(X_train, y_train)\n",
    "baseline.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives: 17\n",
      "true_negatives: 3552\n",
      "false_positives: 39\n",
      "false_negatives: 120\n"
     ]
    }
   ],
   "source": [
    "baseline = gaussian_clf(threshold=0.35, normalize = True)\n",
    "baseline.train(X_train, y_train)\n",
    "baseline.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives: 0\n",
      "true_negatives: 3588\n",
      "false_positives: 3\n",
      "false_negatives: 137\n"
     ]
    }
   ],
   "source": [
    "baseline = gaussian_clf(threshold=0.5, normalize=True)\n",
    "baseline.train(X_train, y_train)\n",
    "baseline.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives: 4\n",
      "true_negatives: 3572\n",
      "false_positives: 19\n",
      "false_negatives: 133\n"
     ]
    }
   ],
   "source": [
    "baseline = gaussian_clf(threshold=0.2, normalize=True)\n",
    "baseline.train(X_train, y_train)\n",
    "baseline.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.gamma(3,3, size =(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07539789, -4.05138487,  4.12678276])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2] - a[2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[2,3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, experiment_names=['C1', 'D4'], val_ratio=0.1, test_ratio=0.1, seed=8):\n",
    "        ci = ChipsIndex()\n",
    "\n",
    "        # all_experiments = ci.experiments()\n",
    "        # EXP_NAMES = ['A1', 'A2', 'A3', 'A4', 'B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'C1', 'D1', 'D2', 'D3', 'D4',\n",
    "        # 'E1', 'E2', 'E3', 'E4', 'E5']\n",
    "        # num_img = 0\n",
    "\n",
    "        training_split = []\n",
    "        testing_split = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Obtain experiment data, combine C1 and D4 training and test sets:\n",
    "        for EXP_NAME in experiment_names:\n",
    "            training_split.extend(ci.training_split_for(EXP_NAME))\n",
    "            testing_split.extend(ci.testing_split_for(EXP_NAME))\n",
    "            labels = ci.labels_for(EXP_NAME)\n",
    "            label_list = list(labels['trn'])\n",
    "            label_list.extend(list(labels['tst']))\n",
    "            all_labels.extend(label_list)\n",
    "\n",
    "        training_split.extend(testing_split)\n",
    "        full_dataset = training_split\n",
    "\n",
    "        # Shuffle the data\n",
    "        ordering = np.arange(len(full_dataset))\n",
    "        rng = np.random.default_rng(seed=seed)\n",
    "        rng.shuffle(ordering)\n",
    "        full_dataset = [full_dataset[i] for i in ordering]\n",
    "        self.all_labels = [all_labels[i] for i in ordering]\n",
    "\n",
    "        # Create training, validation and test sets\n",
    "        self.validation_set = full_dataset[0:int(val_ratio * (len(full_dataset)))]\n",
    "        self.testing_set = full_dataset[\n",
    "                           int(val_ratio * (len(full_dataset))):int((val_ratio + test_ratio) * (len(full_dataset)))]\n",
    "        self.training_set = full_dataset[int((val_ratio + test_ratio) * (len(full_dataset))):]\n",
    "\n",
    "    def get_training_set(self):\n",
    "        return self.training_set\n",
    "\n",
    "    def get_validation_set(self):\n",
    "        return self.validation_set\n",
    "\n",
    "    def get_testing_set(self):\n",
    "        return self.testing_set\n",
    "\n",
    "    def get_full_dataset(self):\n",
    "        return self.training_set + self.validation_set + self.testing_set\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pyvov import ChipsIndex\n",
    "from random import shuffle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_split = []\n",
    "testing_split = []\n",
    "all_labels = []\n",
    "\n",
    "ci = ChipsIndex()\n",
    "experiment_names = ['C1']\n",
    "\n",
    "# Obtain experiment data, combine C1 and D4 training and test sets:\n",
    "for EXP_NAME in experiment_names:\n",
    "    training_split.extend(ci.training_split_for(EXP_NAME))\n",
    "    testing_split.extend(ci.testing_split_for(EXP_NAME))\n",
    "    labels = ci.labels_for(EXP_NAME)\n",
    "    label_list = list(labels['trn'])\n",
    "    label_list.extend(list(labels['tst']))\n",
    "    all_labels.extend(label_list)\n",
    "\n",
    "training_split.extend(testing_split)\n",
    "full_dataset = training_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225,)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16608"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ci.testing_split_for('C1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_experiment(exp_name='A1'):\n",
    "    ci = ChipsIndex()\n",
    "\n",
    "    X_train = np.array(ci.training_split_for(exp_name))\n",
    "    X_test = np.array(ci.testing_split_for(exp_name))\n",
    "\n",
    "    labels_train = np.array(ci.labels_for(exp_name)['trn'])\n",
    "    labels_test = np.array(ci.labels_for(exp_name)['tst'])\n",
    "    \n",
    "    ## make labels \n",
    "    y_train = np.zeros_like(labels_train)\n",
    "    for idx, item in enumerate(labels_train):\n",
    "        if item > 0:\n",
    "            y_train[idx] += 1\n",
    "\n",
    "    y_test = np.zeros_like(labels_test)\n",
    "    for idx, item in enumerate(labels_test):\n",
    "        if item > 0:\n",
    "            y_test[idx] += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    set thresholds for evaluation as you like -- these are referred to as\n",
    "    operating points in the paper.\n",
    "    \"\"\"\n",
    "    baseline = gaussian_clf(threshold=0.75, normalize=False)\n",
    "    baseline.train(X_train, y_train)\n",
    "    baseline.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives: 7\n",
      "true_negatives: 411\n",
      "false_positives: 6\n",
      "false_negatives: 23\n",
      "detected volcanoes: 0.23333333333333334\n"
     ]
    }
   ],
   "source": [
    "evaluate_experiment('A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives: 12\n",
      "true_negatives: 366\n",
      "false_positives: 6\n",
      "false_negatives: 21\n",
      "detected volcanoes: 0.36363636363636365\n"
     ]
    }
   ],
   "source": [
    "evaluate_experiment('A2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives: 30\n",
      "true_negatives: 231\n",
      "false_positives: 8\n",
      "false_negatives: 33\n",
      "detected volcanoes: 0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "evaluate_experiment('A3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_positives: 19\n",
      "true_negatives: 375\n",
      "false_positives: 11\n",
      "false_negatives: 7\n",
      "detected volcanoes: 0.7307692307692307\n"
     ]
    }
   ],
   "source": [
    "evaluate_experiment('A4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
